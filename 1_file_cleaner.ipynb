{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Cleaner\n",
    "\n",
    "#### Note: Use the picasso kernel only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is this used for?\n",
    "- Some imaging channels show sticking of imagers at one sight for 100s of frames. This code will remove the binding events longer than (max_bright_time) allowed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this work? \n",
    "- Link localizations of all the files.\n",
    "- Find localizations with link groups that are longer than the (max_bright_time) allowed.\n",
    "    - One can also append a column in the localizations list with the length of the link group. This can be used in the next step to remove. \n",
    "- Remove the localizations that fall in that link group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Workflow\n",
    "\n",
    "1. Define folder path and the files in the folder. \n",
    "2. Define link localization parameters.\n",
    "    - r_max\n",
    "    - max_dark_time\n",
    "3. Loop through each file and link localizations and filter. \n",
    "    - Link localizations (return link group for each index)\n",
    "    - Count the number of each link group and make another list with the length information. \n",
    "    - Append both the list to the localizations recarray. \n",
    "    - Filter the localizations recarray with the thresholds.\n",
    "4. Save the cleaned files in a separate folder. \n",
    "    - Make a new directory beside the previous directory.\n",
    "    - Save all files with a suffix.\n",
    "    - Save a text file with the amount of data lost. This will give us a record of how much we are losing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "\n",
    "import numpy as _np\n",
    "import os.path as _ospath\n",
    "import os as _os\n",
    "import h5py as _h5py\n",
    "from picasso import lib as _lib\n",
    "from picasso import io as _io\n",
    "from picasso import postprocess as _postprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the folder location and the file extension inside the folder\n",
    "\n",
    "folder = '' # Folder Location. This can also be the whole field of view, to reduce analysis time. \n",
    "file_extn = '.hdf5'\n",
    "file_names = [f for f in _os.listdir(folder) if f.endswith(file_extn)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output folder\n",
    "\n",
    "parent_folder, working_folder = _ospath.split(folder)\n",
    "output_folder = _ospath.join(parent_folder, working_folder, 'Cleaned')\n",
    "if not _ospath.exists(output_folder):\n",
    "    _os.makedirs(output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions needed for linking\n",
    "\n",
    "def locs_per_link_group(link_group):\n",
    "    \"\"\"\n",
    "    Calculates the number of frames/locs per link group in a link_group\n",
    "    numpy array.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    link_group : numpy 1d array\n",
    "        Link group number for each localization (locs).\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    link_group_n : numpy 1d array\n",
    "        Number of frames per each link group.\n",
    "    \"\"\"\n",
    "    # Count occurrences of each link group\n",
    "    unique, counts = _np.unique(link_group, return_counts=True)\n",
    "    frames_per_link_group_dic = dict(zip(unique, counts))\n",
    "    \n",
    "    # Map counts back to the original link_group array\n",
    "    link_group_n = _np.vectorize(frames_per_link_group_dic.get)(link_group).astype(_np.int32)\n",
    "    return link_group_n\n",
    "\n",
    "def save_locs_withSuffix(path, locs, info, suffix=''):\n",
    "    locs = _lib.ensure_sanity(locs, info)\n",
    "    base, ext_locs = _ospath.splitext(path)\n",
    "    output_locs_path = base + '_' + suffix + ext_locs    \n",
    "    output_info_path = base + '_' + suffix + '.yaml'\n",
    "    with _h5py.File(output_locs_path, \"w\") as locs_file:\n",
    "        locs_file.create_dataset(\"locs\", data=locs)\n",
    "    _io.save_info(output_info_path, info, default_flow_style=False)\n",
    "\n",
    "def link_custom(\n",
    "    locs,\n",
    "    r_max,\n",
    "    max_dark_time,\n",
    "):\n",
    "    if len(locs) == 0:\n",
    "        linked_locs = locs.copy()\n",
    "        if hasattr(locs, \"frame\"):\n",
    "            linked_locs = _lib.append_to_rec(\n",
    "                linked_locs, _np.array([], dtype=_np.int32), \"len\"\n",
    "            )\n",
    "            linked_locs = _lib.append_to_rec(\n",
    "                linked_locs, _np.array([], dtype=_np.int32), \"n\"\n",
    "            )\n",
    "        if hasattr(locs, \"photons\"):\n",
    "            linked_locs = _lib.append_to_rec(\n",
    "                linked_locs, _np.array([], dtype=_np.float32), \"photon_rate\"\n",
    "            )\n",
    "    else:\n",
    "        locs.sort(kind=\"mergesort\", order=\"frame\")\n",
    "        if hasattr(locs, \"group\"):\n",
    "            group = locs.group\n",
    "        else:\n",
    "            group = _np.zeros(len(locs), dtype=_np.int32)\n",
    "        \n",
    "        link_group = _postprocess.get_link_groups(locs, r_max, max_dark_time, group)\n",
    "\n",
    "    link_group_n = locs_per_link_group(link_group)\n",
    "        \n",
    "    return link_group_n\n",
    "\n",
    "def locs_cleaner(locs, r_max, max_dark_time, max_bright_time):\n",
    "    link_group_n = link_custom(locs, r_max, max_dark_time)\n",
    "    if len(locs) != len(link_group_n):\n",
    "        raise ValueError(\"Number of events are not matching.\")\n",
    "    mask = _np.array(link_group_n) <= max_bright_time\n",
    "    locs_cleaned = locs[mask]\n",
    "    return locs_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the linking parameters\n",
    "\n",
    "r_max = 1 # Radius/Distance between two consecutive events to link.\n",
    "max_dark_time = 10 # Maximum number of dark frames between two events to link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up parameters\n",
    "\n",
    "max_bright_time = 15 # Discard events longer that specified number of frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleanup\n",
    "\n",
    "for file in file_names:\n",
    "    fpath = _ospath.join(folder, file)\n",
    "    locs, info = _io.load_locs(fpath)\n",
    "    protein_name = file.split('.')[0]\n",
    "    print('Locs loaded for {} channel.'.format(protein_name))\n",
    "    locs_cleaned = locs_cleaner(locs, r_max, max_dark_time, max_bright_time)\n",
    "    percent_data_removed = 100 - (len(locs_cleaned)/len(locs))*100\n",
    "    print('Percent data removed for {} channel is {}%'.format(protein_name, percent_data_removed))\n",
    "    output_path = _ospath.join(output_folder, file)\n",
    "    save_locs_withSuffix(output_path, locs_cleaned, info, suffix='cleaned')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "picasso",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
