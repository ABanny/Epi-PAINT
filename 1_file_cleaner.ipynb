{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Cleaner\n",
    "\n",
    "#### Note: Use the picasso kernel only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is this used for?\n",
    "- Some imaging channels show sticking of imagers at one sight for 100s of frames. This code will remove the binding events longer that (max_bright_time) allowed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this work? \n",
    "- Link localizations of all the files.\n",
    "- Find localizations with link groups that are longer than the (max_bright_time) allowed.\n",
    "    - One can also append a column in the localizations list with the length of the link group. This can be used in the next step to remove. \n",
    "- Remove the localizations that fall in that link group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Workflow\n",
    "\n",
    "1. Define the cell folder. \n",
    "2. Define link localization parameters.\n",
    "    - r_max\n",
    "    - max_dark_time\n",
    "3. Loop through each file and link localizations and filter. \n",
    "    - Link localizations (return link group for each index)\n",
    "    - Count the number of each link group and make another list with the length information. \n",
    "    - Append both the list to the localizations recarray. \n",
    "    - Filter the localizations recarray with the thresholds.\n",
    "4. Save the cleaned files in a separate folder. \n",
    "    - Make a new directory beside the previous directory.\n",
    "    - Save all files with a suffix.\n",
    "    - Save a text file with the amount of data lost. This will give us a record of how much we are losing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "\n",
    "import numpy as _np\n",
    "import os.path as _ospath\n",
    "import os as _os\n",
    "import pandas as _pd\n",
    "from picasso import lib as _lib\n",
    "from picasso import io as _io\n",
    "from picasso import postprocess as _postprocess\n",
    "import epi_paint_picasso_utilis as eppu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the folder location and the file extension inside the folder\n",
    "\n",
    "folder = '' # <<< Set your folder path here\n",
    "file_extn = '.hdf5'\n",
    "file_names = [f for f in _os.listdir(folder) if f.endswith(file_extn)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the linking parameters\n",
    "\n",
    "r_max = 1\n",
    "max_dark_time = 10\n",
    "\n",
    "# Clean up parameters\n",
    "\n",
    "max_bright_time = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output folder\n",
    "\n",
    "parent_folder, working_folder = _ospath.split(folder)\n",
    "output_folder = _ospath.join(parent_folder, working_folder, 'Cleaned')\n",
    "if not _ospath.exists(output_folder):\n",
    "    _os.makedirs(output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Linking functions\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "def locs_per_link_group(link_group):\n",
    "    unique, counts = _np.unique(link_group, return_counts=True)\n",
    "    lookup = dict(zip(unique, counts))\n",
    "    return _np.vectorize(lookup.get)(link_group).astype(_np.int32)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Main linking function (fully fixed)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "def link_custom(locs, r_max, max_dark_time):\n",
    "    # Always convert DataFrame â†’ structured array\n",
    "    locs = eppu.ensure_numpy_structured(locs)\n",
    "\n",
    "    if len(locs) == 0:\n",
    "        # return an empty but valid structured array with appended fields\n",
    "        linked_locs = locs.copy()\n",
    "\n",
    "        if \"frame\" in linked_locs.dtype.names:\n",
    "            linked_locs = _lib.append_to_rec(\n",
    "                linked_locs, _np.array([], dtype=_np.int32), \"len\"\n",
    "            )\n",
    "            linked_locs = _lib.append_to_rec(\n",
    "                linked_locs, _np.array([], dtype=_np.int32), \"n\"\n",
    "            )\n",
    "\n",
    "        if \"photons\" in linked_locs.dtype.names:\n",
    "            linked_locs = _lib.append_to_rec(\n",
    "                linked_locs, _np.array([], dtype=_np.float32), \"photon_rate\"\n",
    "            )\n",
    "\n",
    "        # no link groups\n",
    "        return _np.zeros(0, dtype=_np.int32)\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # FIXED: sorting by frame (for structured arrays)\n",
    "    # ---------------------------------------------------------\n",
    "    locs.sort(order=\"frame\", kind=\"mergesort\")\n",
    "\n",
    "    # group field handling\n",
    "    if \"group\" in locs.dtype.names:\n",
    "        group = locs[\"group\"]\n",
    "    else:\n",
    "        group = _np.zeros(len(locs), dtype=_np.int32)\n",
    "\n",
    "    # compute link groups\n",
    "    frames = locs[\"frame\"]\n",
    "    x = locs[\"x\"]\n",
    "    y = locs[\"y\"]\n",
    "    link_group = _postprocess.get_link_groups(frames, x, y, r_max, max_dark_time, group)\n",
    "\n",
    "    # Count how many frames per link group\n",
    "    return locs_per_link_group(link_group)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Cleaning wrapper\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "def locs_cleaner(locs, r_max, max_dark_time, max_bright_time):\n",
    "    # Ensure consistent structured array\n",
    "    locs = ensure_numpy_structured(locs)\n",
    "\n",
    "    link_group_n = link_custom(locs, r_max, max_dark_time)\n",
    "\n",
    "    if len(locs) != len(link_group_n):\n",
    "        raise ValueError(\"Number of events are not matching.\")\n",
    "\n",
    "    # Filter events by max on-time frames\n",
    "    mask = _np.array(link_group_n) <= max_bright_time\n",
    "\n",
    "    return locs[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleanup\n",
    "\n",
    "for file in file_names:\n",
    "    fpath = _ospath.join(folder, file)\n",
    "    locs, info = _io.load_locs(fpath)\n",
    "    protein_name = file.split('.')[0]\n",
    "    print('Locs loaded for {} channel.'.format(protein_name))\n",
    "    locs_cleaned = locs_cleaner(locs, r_max, max_dark_time, max_bright_time)\n",
    "    percent_data_removed = 100 - (len(locs_cleaned)/len(locs))*100\n",
    "    print('Percent data removed for {} channel is {}%'.format(protein_name, percent_data_removed))\n",
    "    output_path = _ospath.join(output_folder, file)\n",
    "    eppu.save_locs_withSuffix(output_path, locs_cleaned, info, suffix='cleaned')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "picasso",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
